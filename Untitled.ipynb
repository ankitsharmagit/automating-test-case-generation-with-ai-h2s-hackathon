{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c115bcd-f5d3-4e32-bc83-f0baeac04320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "\n",
    "from app.utils import Utils, get_logger\n",
    "from app import (\n",
    "    BatchParser,\n",
    "    RequirementBuilder,\n",
    "    MetadataEnricher,\n",
    "    CategorizerRetriever,\n",
    "    TestCaseGenerator,\n",
    "    CoverageValidator,\n",
    "    SemanticValidator,\n",
    "    RegulationMapper,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Initialize Utils + Logger\n",
    "# -------------------------\n",
    "utils = Utils()\n",
    "logger = get_logger(\"Pipeline\", log_file=\"logs/pipeline.log\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 1 – Requirement Ingestion\n",
    "# -------------------------------\n",
    "def parse_all_requirements(data_folder=\"data\"):\n",
    "    parser = BatchParser(data_folder=data_folder)\n",
    "    results = parser.parse_batch()\n",
    "    all_reqs = [req for _, reqs in results.items() for req in reqs]\n",
    "    logger.info(f\"Parsed {len(all_reqs)} total requirements from {len(results)} files.\")\n",
    "    return all_reqs\n",
    "\n",
    "\n",
    "def run_requirement_builder(\n",
    "    project_id,\n",
    "    llm_name,\n",
    "    output_file,\n",
    "    batch_size=30,\n",
    "):\n",
    "    logger.info(\"Step 0: Parsing raw requirements...\")\n",
    "    all_req = parse_all_requirements()\n",
    "\n",
    "    if not all_req:\n",
    "        logger.warning(\"No requirements found. Exiting pipeline.\")\n",
    "        return []\n",
    "\n",
    "    logger.info(\"Step 1: Building structured requirements...\")\n",
    "    builder = RequirementBuilder(model=llm_name, project_id=project_id)\n",
    "    # structured_reqs = asyncio.run(builder.build_registry(all_req, batch_size=batch_size))\n",
    "    structured_reqs = await builder.build_registry(all_req, batch_size=batch_size)\n",
    "    logger.info(f\"Built {len(structured_reqs)} structured requirements\")\n",
    "\n",
    "    if structured_reqs:\n",
    "        utils.save_json(structured_reqs, output_file)\n",
    "    return structured_reqs\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 3 – Metadata Enrichment\n",
    "# -------------------------------\n",
    "# def load_requirements_from_bigquery(project_id, dataset_id, table_id):\n",
    "#     structured_reqs = utils.read_from_bigquery(project_id, dataset_id, table_id)\n",
    "#     for req in structured_reqs:\n",
    "#         if isinstance(req.get(\"metadata\"), str):\n",
    "#             try:\n",
    "#                 req[\"metadata\"] = json.loads(req[\"metadata\"])\n",
    "#             except Exception:\n",
    "#                 req[\"metadata\"] = {}\n",
    "#     logger.info(f\"Loaded {len(structured_reqs)} requirements from BigQuery.\")\n",
    "#     return structured_reqs\n",
    "\n",
    "\n",
    "def enrich_requirements(requirements, output_file):\n",
    "    enricher = MetadataEnricher()\n",
    "    enriched_reqs = enricher.enrich(requirements)\n",
    "    if enriched_reqs:\n",
    "        utils.save_json(enriched_reqs, output_file)\n",
    "    return enriched_reqs\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 4 – Categorization & Retrieval\n",
    "# -------------------------------\n",
    "async def categorize_and_store_requirements(\n",
    "    project_id,\n",
    "    llm_model,\n",
    "    enriched_file,\n",
    "    embedding_model,\n",
    "    dataset_id,\n",
    "    table_id,\n",
    "    batch_size=10,\n",
    "):\n",
    "    enriched_reqs = utils.read_json(enriched_file)\n",
    "\n",
    "    cr = CategorizerRetriever(\n",
    "        project_id=project_id,\n",
    "        embedding_model=embedding_model,\n",
    "        classifier_model=llm_model,\n",
    "    )\n",
    "\n",
    "    categorized_reqs = await cr.process_async(enriched_reqs, batch_size)\n",
    "    cr.export_to_bq(categorized_reqs, dataset_id=dataset_id, table_id=table_id)\n",
    "    return categorized_reqs\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 5 – Test Case Generation\n",
    "# -------------------------------\n",
    "async def generate_and_store_test_cases(\n",
    "    requirements,\n",
    "    model,\n",
    "    output_file,\n",
    "    batch_size=100,\n",
    "):\n",
    "    # # client = bigquery.Client(project=project_id)\n",
    "    # query = f\"\"\"\n",
    "    #     SELECT requirement_id, category, title, statement, priority, severity\n",
    "    #     FROM `{project_id}.{dataset_id}.{requirements_table}`\n",
    "    # \"\"\"\n",
    "    # rows = client.query(query).result()\n",
    "    # requirements = [dict(row) for row in rows]\n",
    "\n",
    "    tcg = TestCaseGenerator(model=model)\n",
    "    test_cases = await tcg.batch_generate_async(requirements, batch_size=batch_size)\n",
    "    \n",
    "    # tcg.export_to_bq(\n",
    "    #     test_cases,\n",
    "    #     dataset_id=dataset_id,\n",
    "    #     table_id=testcases_table,\n",
    "    #     batch_size=export_batch_size,\n",
    "    # )\n",
    "\n",
    "    if test_cases:\n",
    "        utils.save_json(test_cases, output_file)\n",
    "    return test_cases\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Regulation Mapping\n",
    "# -------------------------------\n",
    "async def run_regulation_mapping(requirements, model, output_file, batch_size=10,):\n",
    "    mapper = RegulationMapper(regulation_file=\"regulations.yaml\", model=model)\n",
    "    texts = [req.get(\"statement\", \"\") for req in requirements]\n",
    "    mapped = await mapper.map_batch(texts, batch_size=batch_size)\n",
    "\n",
    "    for req, m in zip(requirements, mapped):\n",
    "        req[\"regulation\"] = m.get(\"regulation\", [])\n",
    "        req[\"obligations\"] = m.get(\"obligations\", [])\n",
    "    if requirements :\n",
    "        utils.save_json(requirements, output_file)\n",
    "        \n",
    "    return requirements\n",
    "\n",
    "\n",
    "def count_requirements_by_regulation(mapped_reqs):\n",
    "    counts = {}\n",
    "    for req in mapped_reqs:\n",
    "        regs = req.get(\"regulation\", [])\n",
    "        if isinstance(regs, str):\n",
    "            regs = [regs]\n",
    "        for reg in regs:\n",
    "            counts[reg] = counts.get(reg, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def show_regulation_coverage(mapped_reqs, logger=None):\n",
    "    counts = count_requirements_by_regulation(mapped_reqs)\n",
    "    total = len(mapped_reqs)\n",
    "    if logger:\n",
    "        logger.info(\"Regulation Coverage Summary:\")\n",
    "        for reg, count in sorted(counts.items(), key=lambda x: -x[1]):\n",
    "            pct = (count / total) * 100 if total else 0\n",
    "            logger.info(f\" - {reg}: {count}/{total} ({pct:.1f}%)\")\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e30fc8-fb00-4945-a37a-7c40db0d9005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Global Config & Constants\n",
    "# -------------------------\n",
    "client = bigquery.Client()\n",
    "PROJECT_ID = client.project\n",
    "LOCATION = \"us-central1\"\n",
    "DATASET_ID = \"requirements_dataset\"\n",
    "\n",
    "# -------------------------\n",
    "# Model Configs\n",
    "# -------------------------\n",
    "LLM_MODEL = \"gemini-2.5-pro\"\n",
    "EMBEDDING_MODEL = \"text-embedding-005\"\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "RAW_REQ_FILE = \"requirements.json\"\n",
    "STRUCTURED_REQ_FILE = \"structured_requirements.json\"\n",
    "ENRICHED_REQ_FILE = \"enriched_requirements.json\"\n",
    "MAPPED_REQ_FILE = \"regulation_mapped_requirements.json\"\n",
    "TEST_CASES_FILE = \"test_cases.json\"\n",
    "\n",
    "# -------------------------\n",
    "# BQ Table Names\n",
    "# -------------------------\n",
    "REQ_TABLE = \"requirements\"\n",
    "TC_TABLE = \"test_cases\"\n",
    "TRACE_TABLE = \"traceability_matrix\"\n",
    "SEMANTIC_TABLE = \"semantic_validation\"\n",
    "\n",
    "# -------------------------\n",
    "# Environment Vars\n",
    "# -------------------------\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc03d71c-ae5d-444e-824b-2f9a7b2887e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 13:30:13,021 [INFO] Pipeline: Step 0: Parsing raw requirements...\n",
      "INFO:Pipeline:Step 0: Parsing raw requirements...\n",
      "2025-09-03 13:30:13,025 [INFO] BatchParser: Initialized BatchParser with data_folder=data\n",
      "INFO:BatchParser:Initialized BatchParser with data_folder=data\n",
      "2025-09-03 13:30:13,028 [INFO] BatchParser: Collected 1 supported files from data\n",
      "2025-09-03 13:30:13,030 [INFO] BatchParser: Parsing file: data/Common_InsuranceReqs_FINAL.pdf\n",
      "INFO:BatchParser:Collected 1 supported files from data\n",
      "INFO:BatchParser:Parsing file: data/Common_InsuranceReqs_FINAL.pdf\n",
      "2025-09-03 13:30:16,219 [INFO] BatchParser: Extracted 85 requirements from Common_InsuranceReqs_FINAL.pdf\n",
      "2025-09-03 13:30:16,221 [INFO] BatchParser: Finished parsing batch → 1 files processed\n",
      "INFO:BatchParser:Extracted 85 requirements from Common_InsuranceReqs_FINAL.pdf\n",
      "2025-09-03 13:30:16,223 [INFO] Pipeline: Parsed 85 total requirements from 1 files.\n",
      "INFO:BatchParser:Finished parsing batch → 1 files processed\n",
      "2025-09-03 13:30:16,225 [INFO] Pipeline: Step 1: Building structured requirements...\n",
      "INFO:Pipeline:Parsed 85 total requirements from 1 files.\n",
      "INFO:Pipeline:Step 1: Building structured requirements...\n",
      "2025-09-03 13:30:16,239 [INFO] RequirementBuilder: Initialized RequirementBuilder with project_id=second-sandbox-470608-m2, model=gemini-2.5-pro, location=us-central1\n",
      "INFO:RequirementBuilder:Initialized RequirementBuilder with project_id=second-sandbox-470608-m2, model=gemini-2.5-pro, location=us-central1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Build structured requirements\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m structured_reqs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_requirement_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROJECT_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mLLM_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTRUCTURED_REQ_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m, in \u001b[0;36mrun_requirement_builder\u001b[0;34m(project_id, llm_name, output_file, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 1: Building structured requirements...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m builder \u001b[38;5;241m=\u001b[39m RequirementBuilder(model\u001b[38;5;241m=\u001b[39mllm_name, project_id\u001b[38;5;241m=\u001b[39mproject_id)\n\u001b[0;32m---> 54\u001b[0m structured_reqs \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_registry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_req\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(structured_reqs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m structured requirements\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m structured_reqs:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# Step 1: Build structured requirements\n",
    "structured_reqs = run_requirement_builder(PROJECT_ID,\n",
    "                                          LLM_MODEL, \n",
    "                                          output_file=STRUCTURED_REQ_FILE,\n",
    "                                          batch_size=100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12430968-787d-477d-9a61-09894cc0af60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e98d3d-b064-4168-a905-00917b889e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Main Entrypoint\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    # LLM + embeddings\n",
    "    # llm = VertexAI(model_name=LLM_MODEL, temperature=0, project=PROJECT_ID, location=LOCATION)\n",
    "    # embedding_model = VertexAIEmbeddings(model=EMBEDDING_MODEL, project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "   \n",
    "\n",
    "    # Step 2: Regulation Mapping\n",
    "    structured_reqs=utils.read_json(STRUCTURED_REQ_FILE)\n",
    "    regulation_mapped_reqs = asyncio.run(run_regulation_mapping(structured_reqs,                             \n",
    "                                                                model=LLM_MODEL,\n",
    "                                                                output_file=MAPPED_REQ_FILE,\n",
    "                                                                batch_size=100,))\n",
    "    show_regulation_coverage(regulation_mapped_reqs, logger)\n",
    "\n",
    "    # Step 3: Metadata Enrichment\n",
    "    enriched_reqs = enrich_requirements(regulation_mapped_reqs, output_file=ENRICHED_REQ_FILE)\n",
    "    \n",
    "\n",
    "    # Step 4: Test Case Generation\n",
    "    enriched_reqs=utils.read_json(ENRICHED_REQ_FILE)\n",
    "    limit=5\n",
    "    if limit:\n",
    "        enriched_reqs = enriched_reqs[:limit]\n",
    "        \n",
    "        \n",
    "    logger.info(f\"{len(enriched_reqs)} requirements loaded for test case generation\")\n",
    "    test_cases = asyncio.run(generate_and_store_test_cases(enriched_reqs, \n",
    "                                                            model=LLM_MODEL,\n",
    "                                                            output_file=TEST_CASES_FILE,\n",
    "                                                            batch_size=100))\n",
    "\n",
    "    # Step 5: Load Requirements and Testcases to BigQuery\n",
    "    utils.load_requirements_to_bq(RAW_REQ_FILE, REQ_TABLE, project_id=PROJECT_ID, dataset_id=DATASET_ID)\n",
    "    utils.load_testcases_to_bq(TEST_CASES_FILE, TC_TABLE, project_id=PROJECT_ID, dataset_id=DATASET_ID)\n",
    "\n",
    "\n",
    "    # Step 6: Traceability Matrix\n",
    "    cv = CoverageValidator(project_id=PROJECT_ID, dataset_id=DATASET_ID)\n",
    "    trace_matrix = cv.build_traceability_matrix(\n",
    "        requirements_table=REQ_TABLE,\n",
    "        testcases_table=TC_TABLE,\n",
    "        output_table=TRACE_TABLE,\n",
    "    )\n",
    "    df_trace = pd.DataFrame(trace_matrix)\n",
    "    print(df_trace.head())\n",
    "\n",
    "    # Step 7: Semantic Validation\n",
    "    enriched_reqs = utils.read_json(ENRICHED_REQ_FILE)\n",
    "    test_cases = utils.read_json(TEST_CASES_FILE)\n",
    "    sv = SemanticValidator(project_id=PROJECT_ID)\n",
    "    validated = asyncio.run(\n",
    "        sv.validate_async(enriched_reqs, test_cases, batch_size=20)\n",
    "    )\n",
    "    sv.export_to_bq(validated, table_id=SEMANTIC_TABLE)\n",
    "    df_validated = pd.DataFrame(validated)\n",
    "    print(df_validated.head())\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
