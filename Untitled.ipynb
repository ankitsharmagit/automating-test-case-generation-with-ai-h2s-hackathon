{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64e491d-b509-4d76-b763-e2be59447d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import bigquery\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from collections import Counter\n",
    "from app.utils import save_json,read_json\n",
    "from collections import Counter\n",
    "\n",
    "from app import (\n",
    "    BatchParser,\n",
    "    RequirementBuilder,\n",
    "    MetadataEnricher,\n",
    "    CategorizerRetriever,\n",
    "    TestCaseGenerator,\n",
    "    CoverageValidator,\n",
    "    SemanticValidator,\n",
    "    RegulationMapper\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Global Config\n",
    "# -------------------------------\n",
    "client = bigquery.Client()\n",
    "PROJECT_ID = client.project\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "LLM_MODEL = \"gemini-2.5-pro\"\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 1 ‚Äì Requirement Ingestion\n",
    "# -------------------------------\n",
    "from app.batch_parser import BatchParser\n",
    "\n",
    "def parse_all_requirements(data_folder=\"data\"):\n",
    "    \"\"\"\n",
    "    Reads all supported files from the given folder,\n",
    "    parses them, and returns a single combined list of requirements.\n",
    "    \"\"\"\n",
    "    parser = BatchParser(data_folder=data_folder)\n",
    "    results = parser.parse_batch()\n",
    "\n",
    "    # Flatten results into one list\n",
    "    all_reqs = []\n",
    "    for _, reqs in results.items():\n",
    "        all_reqs.extend(reqs)\n",
    "\n",
    "    print(f\"‚úÖ Parsed {len(all_reqs)} total requirements from {len(results)} files.\")\n",
    "    return all_reqs\n",
    "\n",
    "def run_requirement_builder(project_id, model=\"gemini-2.5-pro\", batch_size=30, save_file=\"structured_requirements.json\"):\n",
    "    \"\"\"\n",
    "    Full pipeline to parse raw requirements and build structured requirements.\n",
    "    \n",
    "    Steps:\n",
    "    1. Parse all raw requirements from 'data/' folder\n",
    "    2. Use LLM to build structured requirements\n",
    "    3. Save structured requirements locally as JSON\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Step 0: Parsing raw requirements...\")\n",
    "    all_req = parse_all_requirements()\n",
    "    print(f\"   ‚Üí Parsed {len(all_req)} raw requirements\")\n",
    "\n",
    "    if not all_req:\n",
    "        print(\"‚ö†Ô∏è No requirements found. Exiting pipeline.\")\n",
    "        return []\n",
    "\n",
    "    print(\"üöÄ Step 1: Building structured requirements...\")\n",
    "    builder = RequirementBuilder(model=model, project_id=project_id)\n",
    "\n",
    "    structured_reqs = asyncio.run(builder.build_registry(all_req, batch_size=batch_size))\n",
    "    print(f\"   ‚Üí Built {len(structured_reqs)} structured requirements\")\n",
    "\n",
    "    # Save locally\n",
    "    if structured_reqs:\n",
    "        save_json(structured_reqs, save_file)\n",
    "        print(f\"üíæ Saved structured requirements ‚Üí {save_file}\")\n",
    "\n",
    "    return structured_reqs\n",
    "# -------------------------------\n",
    "# Layer 3 ‚Äì Metadata Enrichment\n",
    "# -------------------------------\n",
    "def load_requirements_from_bigquery(project_id, dataset_id=\"requirements_dataset\", table_id=\"requirements\"):\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    query = f\"SELECT * FROM `{table_ref}`\"\n",
    "    rows = client.query(query).result()\n",
    "    structured_reqs = [dict(row) for row in rows]\n",
    "\n",
    "    for req in structured_reqs:\n",
    "        if isinstance(req.get(\"metadata\"), str):\n",
    "            try:\n",
    "                req[\"metadata\"] = json.loads(req[\"metadata\"])\n",
    "            except Exception:\n",
    "                req[\"metadata\"] = {}\n",
    "\n",
    "    print(f\"Loaded {len(structured_reqs)} requirements from BigQuery.\")\n",
    "    return structured_reqs\n",
    "\n",
    "\n",
    "def enrich_requirements(requirements, output_file=\"enriched_requirements.json\"):\n",
    "    enricher = MetadataEnricher()\n",
    "    enriched_reqs = enricher.enrich(requirements)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(enriched_reqs, f, indent=2,default=str)\n",
    "\n",
    "    print(f\"Enriched {len(enriched_reqs)} requirements ‚Üí {output_file}\")\n",
    "    return enriched_reqs\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 4 ‚Äì Categorization & Retrieval\n",
    "# -------------------------------\n",
    "async def categorize_and_store_requirements(\n",
    "    project_id,\n",
    "    llm_model,\n",
    "    enriched_file=\"enriched_requirements.json\",\n",
    "    embedding_model=\"text-embedding-005\",\n",
    "    dataset_id=\"requirements_dataset\",\n",
    "    table_id=\"requirements_categorized\",\n",
    "    batch_size=10,\n",
    "):\n",
    "    with open(enriched_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        enriched_reqs = json.load(f)\n",
    "\n",
    "    cr = CategorizerRetriever(\n",
    "        project_id=project_id,\n",
    "        embedding_model=embedding_model,\n",
    "        classifier_model=llm_model\n",
    "    )\n",
    "\n",
    "    categorized_reqs = await cr.process_async(enriched_reqs, batch_size)\n",
    "    cr.export_to_bq(categorized_reqs, dataset_id=dataset_id, table_id=table_id)\n",
    "\n",
    "    print(f\"Processed {len(categorized_reqs)} requirements ‚Üí exported to {dataset_id}.{table_id}\")\n",
    "    return categorized_reqs\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 5 ‚Äì Test Case Generation\n",
    "# -------------------------------\n",
    "async def generate_and_store_test_cases(\n",
    "    project_id,\n",
    "    dataset_id=\"requirements_dataset\",\n",
    "    requirements_table=\"requirements_categorized\",\n",
    "    testcases_table=\"test_cases\",\n",
    "    save_local=True,\n",
    "    local_file=\"test_cases.json\",\n",
    "    limit=None,\n",
    "    batch_size=100,\n",
    "    export_batch_size=200,\n",
    "):\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    query = f\"\"\"\n",
    "        SELECT requirement_id, category, title, statement, priority, severity\n",
    "        FROM `{project_id}.{dataset_id}.{requirements_table}`\n",
    "    \"\"\"\n",
    "    rows = client.query(query).result()\n",
    "    requirements = [dict(row) for row in rows]\n",
    "\n",
    "    if limit:\n",
    "        requirements = requirements[:limit]\n",
    "\n",
    "    tcg = TestCaseGenerator(project_id)\n",
    "    test_cases = await tcg.batch_generate_async(requirements, batch_size=batch_size)\n",
    "    tcg.export_to_bq(test_cases, dataset_id=dataset_id, table_id=testcases_table, batch_size=export_batch_size)\n",
    "\n",
    "    if save_local:\n",
    "        with open(local_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(test_cases, f, indent=2)\n",
    "\n",
    "    print(f\"Exported {len(test_cases)} test cases to BigQuery ‚Üí {dataset_id}.{testcases_table}\")\n",
    "    return test_cases\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 6 ‚Äì Coverage Validation\n",
    "# -------------------------------\n",
    "def build_traceability_matrix(project_id, dataset_id=\"requirements_dataset\", table_name=\"traceability_matrix\"):\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "    client.delete_table(table_ref, not_found_ok=True)\n",
    "    print(f\"Dropped old table {table_ref}\")\n",
    "\n",
    "    cv = CoverageValidator(project_id=project_id, dataset_id=dataset_id)\n",
    "    trace_matrix = cv.build_traceability_matrix()\n",
    "    df = pd.DataFrame(trace_matrix)\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 6.5 ‚Äì Semantic Validation\n",
    "# -------------------------------\n",
    "async def run_semantic_validation(\n",
    "    project_id,\n",
    "    dataset_id=\"requirements_dataset\",\n",
    "    requirements_table=\"requirements\",\n",
    "    testcases_table=\"test_cases\",\n",
    "    output_table=\"semantic_validation\",\n",
    "    limit=None,\n",
    "    batch_size=10,\n",
    "):\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    req_query = f\"SELECT DISTINCT requirement_id, statement FROM `{project_id}.{dataset_id}.{requirements_table}`\"\n",
    "    requirements = [dict(row) for row in client.query(req_query).result()]\n",
    "\n",
    "    tc_query = f\"SELECT DISTINCT test_id, requirement_id, title, description, steps, expected_result FROM `{project_id}.{dataset_id}.{testcases_table}`\"\n",
    "    test_cases = [dict(row) for row in client.query(tc_query).result()]\n",
    "\n",
    "    if limit:\n",
    "        requirements = requirements[:limit]\n",
    "        test_cases = test_cases[:limit]\n",
    "\n",
    "    sv = SemanticValidator(project_id=project_id)\n",
    "    validated = await sv.validate_async(requirements, test_cases, batch_size=batch_size)\n",
    "    sv.export_to_bq(validated, table_id=output_table)\n",
    "    df = pd.DataFrame(validated)\n",
    "\n",
    "    print(f\"Exported {len(df)} validation results to {dataset_id}.{output_table}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def show_regulation_coverage(enriched_reqs, total_reqs=None, logger=None):\n",
    "    \"\"\"\n",
    "    Show compliance coverage stats.\n",
    "    Calculates percentage of requirements mapped to each regulation.\n",
    "    \n",
    "    Args:\n",
    "        enriched_reqs (list[dict]): List of enriched requirement dicts.\n",
    "        total_reqs (int, optional): Total number of requirements in the project.\n",
    "                                    If None, uses len(enriched_reqs).\n",
    "        logger (Logger, optional): Logger instance. If None, prints to stdout.\n",
    "    \"\"\"\n",
    "    all_regs = [reg for r in enriched_reqs for reg in r.get(\"regulation\", [])]\n",
    "\n",
    "    if all_regs:\n",
    "        counts = Counter(all_regs)\n",
    "        base_total = total_reqs if total_reqs is not None else len(enriched_reqs)\n",
    "\n",
    "        msg_header = \"\\nüìä Regulation Coverage (relative to total requirements):\"\n",
    "        if logger:\n",
    "            logger.info(msg_header)\n",
    "        else:\n",
    "            print(msg_header)\n",
    "\n",
    "        for reg, count in counts.items():\n",
    "            pct = (count / base_total) * 100 if base_total > 0 else 0\n",
    "            msg = f\"   - {reg}: {count}/{base_total} ({pct:.1f}%)\"\n",
    "            if logger:\n",
    "                logger.info(msg)\n",
    "            else:\n",
    "                print(msg)\n",
    "    else:\n",
    "        msg = \"\\n‚ö†Ô∏è No regulations found in enriched requirements.\"\n",
    "        if logger:\n",
    "            logger.warning(msg)\n",
    "        else:\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "async def run_regulation_mapping(requirements, llm, batch_size=10):\n",
    "    \"\"\"\n",
    "    Map regulations & obligations for given requirements.\n",
    "    Updates each requirement with `regulation` and `obligations`.\n",
    "    \"\"\"\n",
    "    # Initialize RegulationMapper with provided LLM\n",
    "    mapper = RegulationMapper(regulation_file=\"regulations.yaml\", llm=llm, project_id=PROJECT_ID)\n",
    "\n",
    "    texts = [req.get(\"statement\", \"\") for req in requirements]\n",
    "    mapped = await mapper.map_batch(texts, batch_size=batch_size)\n",
    "\n",
    "    for req, m in zip(requirements, mapped):\n",
    "        req[\"regulation\"] = m[\"regulation\"]      # column 1\n",
    "        req[\"obligations\"] = m[\"obligations\"]    # column 2\n",
    "\n",
    "    return requirements\n",
    "\n",
    "\n",
    "def count_requirements_by_regulation(mapped_reqs):\n",
    "    \"\"\"\n",
    "    Count how many requirements are mapped to each regulation.\n",
    "    \"\"\"\n",
    "    all_regs = []\n",
    "    for req in mapped_reqs:\n",
    "        regs = req.get(\"regulation\", [])\n",
    "        if isinstance(regs, str):\n",
    "            regs = [regs]\n",
    "        all_regs.extend(regs)\n",
    "\n",
    "    counts = Counter(all_regs)\n",
    "    total_reqs = len(mapped_reqs)\n",
    "\n",
    "    print(\"\\nüìä Regulation Coverage (count of requirements mapped):\")\n",
    "    for reg, count in counts.most_common():\n",
    "        pct = (count / total_reqs) * 100\n",
    "        print(f\" - {reg}: {count}/{total_reqs} ({pct:.1f}%)\")\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3b984-8d64-46c2-8bb5-051c94909da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "  # ---------------- Run Pipeline ----------------\n",
    "print(\"üöÄ Step 1: Building structured requirements...\")\n",
    "file_name=\"structured_requirements.json\"\n",
    "structured_reqs = run_requirement_builder(\n",
    "            project_id=PROJECT_ID,\n",
    "            model=LLM_MODEL,\n",
    "            batch_size=50,\n",
    "            save_file=file_name\n",
    "        )\n",
    "\n",
    "structured_reqs=read_json(file_name)\n",
    "\n",
    "\n",
    "# Load structured requirements (already built & saved earlier)\n",
    "structured_reqs = read_json(\"structured_requirements.json\")\n",
    "\n",
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")\n",
    "# regulation_mapped_reqs = await run_regulation_mapping(structured_reqs, llm=llm, batch_size=50)\n",
    "\n",
    "\n",
    "# counts = count_requirements_by_regulation(regulation_mapped_reqs)\n",
    "# print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324329af-e4d1-4144-aa04-ce3ccd88d9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save enriched mapped_regs\n",
    "# save_json(regulation_mapped_reqs, \"regulation_mapped_reuirements.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "009a3540-b0ed-4fc4-83e0-2fc6508dc380",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 05:46:09,820 [INFO] Utils: ‚úÖ Loaded 73 requirements from regulation_mapped_reuirement.json\n",
      "INFO:Utils:‚úÖ Loaded 73 requirements from regulation_mapped_reuirement.json\n"
     ]
    }
   ],
   "source": [
    "regulation_mapped_reqs = read_json(\"regulation_mapped_reuirements.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f718ad0b-7751-4792-8877-7482a7be4d78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requirement_id</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>statement</th>\n",
       "      <th>priority</th>\n",
       "      <th>severity</th>\n",
       "      <th>regulation</th>\n",
       "      <th>actors</th>\n",
       "      <th>data_type</th>\n",
       "      <th>action</th>\n",
       "      <th>acceptance_criteria</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>traceability</th>\n",
       "      <th>metadata</th>\n",
       "      <th>created_at</th>\n",
       "      <th>obligations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REQ-001</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>Stakeholder Adoption and Interoperability</td>\n",
       "      <td>These requirements have since been used by a v...</td>\n",
       "      <td>P1</td>\n",
       "      <td>Major</td>\n",
       "      <td>[NA]</td>\n",
       "      <td>[System Integrator, Policy Maker, Government B...</td>\n",
       "      <td>[PHI, Health Insurance Claims, Patient Demogra...</td>\n",
       "      <td>[Comply, Interoperate]</td>\n",
       "      <td>[The system's architecture and data exchange p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'source_file': 'Common_InsuranceReqs_FINAL.pd...</td>\n",
       "      <td>2025-09-02T05:19:48.697595</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REQ-003</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>Update National Health Insurance System Requir...</td>\n",
       "      <td>A 'Guide to Common Requirements for National H...</td>\n",
       "      <td>P1</td>\n",
       "      <td>Major</td>\n",
       "      <td>[HIPAA, ISO 27001]</td>\n",
       "      <td>[Compliance Officer, Business Analyst, Project...</td>\n",
       "      <td>[PHI, Insurance Claims Data]</td>\n",
       "      <td>[modify, create]</td>\n",
       "      <td>[A new version of the 'Guide to Common Require...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'source_file': 'Common_InsuranceReqs_FINAL.pd...</td>\n",
       "      <td>2025-09-02T05:19:48.697956</td>\n",
       "      <td>[Update requirements guide, Incorporate worksh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REQ-004</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>Update National Health Insurance Information S...</td>\n",
       "      <td>The 'Guide to Common Requirements for National...</td>\n",
       "      <td>P1</td>\n",
       "      <td>Major</td>\n",
       "      <td>[HIPAA, ISO 27001]</td>\n",
       "      <td>[Compliance Officer, System Architect, Project...</td>\n",
       "      <td>[Requirements Documentation]</td>\n",
       "      <td>[modify]</td>\n",
       "      <td>[A new version of the 'Guide to Common Require...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'source_file': 'Common_InsuranceReqs_FINAL.pd...</td>\n",
       "      <td>2025-09-02T05:19:48.698029</td>\n",
       "      <td>[Update official documentation, Incorporate wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REQ-005</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>Living Document Contribution Process</td>\n",
       "      <td>In addition, this document suggests a way forw...</td>\n",
       "      <td>P2</td>\n",
       "      <td>Major</td>\n",
       "      <td>[NA]</td>\n",
       "      <td>[Insurance Technology Stakeholder, Admin, Comp...</td>\n",
       "      <td>Requirements Document</td>\n",
       "      <td>[contribute, update, review]</td>\n",
       "      <td>[A formal process for submitting proposed upda...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'source_file': 'Common_InsuranceReqs_FINAL.pd...</td>\n",
       "      <td>2025-09-02T05:19:48.698095</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REQ-006</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>National Health Insurance System Context</td>\n",
       "      <td>The system must be designed in the context of ...</td>\n",
       "      <td>P3</td>\n",
       "      <td>Minor</td>\n",
       "      <td>[HIPAA, ISO 27001]</td>\n",
       "      <td>[Patient, Payer, Health Authority, Admin]</td>\n",
       "      <td>[PHI, Financial Data, Insurance Information]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[The system's purpose and design documentation...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'source_file': 'Common_InsuranceReqs_FINAL.pd...</td>\n",
       "      <td>2025-09-02T05:19:48.698161</td>\n",
       "      <td>[Protect personal health information, Secure f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  requirement_id    category  \\\n",
       "0        REQ-001  Compliance   \n",
       "1        REQ-003  Compliance   \n",
       "2        REQ-004  Compliance   \n",
       "3        REQ-005  Compliance   \n",
       "4        REQ-006  Compliance   \n",
       "\n",
       "                                               title  \\\n",
       "0          Stakeholder Adoption and Interoperability   \n",
       "1  Update National Health Insurance System Requir...   \n",
       "2  Update National Health Insurance Information S...   \n",
       "3               Living Document Contribution Process   \n",
       "4           National Health Insurance System Context   \n",
       "\n",
       "                                           statement priority severity  \\\n",
       "0  These requirements have since been used by a v...       P1    Major   \n",
       "1  A 'Guide to Common Requirements for National H...       P1    Major   \n",
       "2  The 'Guide to Common Requirements for National...       P1    Major   \n",
       "3  In addition, this document suggests a way forw...       P2    Major   \n",
       "4  The system must be designed in the context of ...       P3    Minor   \n",
       "\n",
       "           regulation                                             actors  \\\n",
       "0                [NA]  [System Integrator, Policy Maker, Government B...   \n",
       "1  [HIPAA, ISO 27001]  [Compliance Officer, Business Analyst, Project...   \n",
       "2  [HIPAA, ISO 27001]  [Compliance Officer, System Architect, Project...   \n",
       "3                [NA]  [Insurance Technology Stakeholder, Admin, Comp...   \n",
       "4  [HIPAA, ISO 27001]          [Patient, Payer, Health Authority, Admin]   \n",
       "\n",
       "                                           data_type  \\\n",
       "0  [PHI, Health Insurance Claims, Patient Demogra...   \n",
       "1                       [PHI, Insurance Claims Data]   \n",
       "2                       [Requirements Documentation]   \n",
       "3                              Requirements Document   \n",
       "4       [PHI, Financial Data, Insurance Information]   \n",
       "\n",
       "                         action  \\\n",
       "0        [Comply, Interoperate]   \n",
       "1              [modify, create]   \n",
       "2                      [modify]   \n",
       "3  [contribute, update, review]   \n",
       "4                            []   \n",
       "\n",
       "                                 acceptance_criteria dependencies  \\\n",
       "0  [The system's architecture and data exchange p...           []   \n",
       "1  [A new version of the 'Guide to Common Require...           []   \n",
       "2  [A new version of the 'Guide to Common Require...           []   \n",
       "3  [A formal process for submitting proposed upda...           []   \n",
       "4  [The system's purpose and design documentation...           []   \n",
       "\n",
       "  traceability                                           metadata  \\\n",
       "0           []  {'source_file': 'Common_InsuranceReqs_FINAL.pd...   \n",
       "1           []  {'source_file': 'Common_InsuranceReqs_FINAL.pd...   \n",
       "2           []  {'source_file': 'Common_InsuranceReqs_FINAL.pd...   \n",
       "3           []  {'source_file': 'Common_InsuranceReqs_FINAL.pd...   \n",
       "4           []  {'source_file': 'Common_InsuranceReqs_FINAL.pd...   \n",
       "\n",
       "                   created_at  \\\n",
       "0  2025-09-02T05:19:48.697595   \n",
       "1  2025-09-02T05:19:48.697956   \n",
       "2  2025-09-02T05:19:48.698029   \n",
       "3  2025-09-02T05:19:48.698095   \n",
       "4  2025-09-02T05:19:48.698161   \n",
       "\n",
       "                                         obligations  \n",
       "0                                                 []  \n",
       "1  [Update requirements guide, Incorporate worksh...  \n",
       "2  [Update official documentation, Incorporate wo...  \n",
       "3                                                 []  \n",
       "4  [Protect personal health information, Secure f...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(regulation_mapped_reqs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a342c76-a8f2-4a06-a04f-6d6cc303163a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416e98f-ddc3-4696-9dc4-737d828b60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     print(\"üîç Step 2: Enriching requirements with metadata...\")\n",
    "#     enriched_reqs = enrich_requirements(structured_reqs)\n",
    "#     from app.compliance_validator import ComplianceValidator\n",
    "#     from app.utils import save_json\n",
    "\n",
    "#     # Load requirements (after enrichment step)\n",
    "#     requirements = read_json(\"enriched_requirements.json\")\n",
    "#     # print(requirements[0])\n",
    "\n",
    "#     validator = ComplianceValidator(\"regulations.yaml\")\n",
    "#     enriched_reqs, compliance_summary = validator.validate(requirements)\n",
    "\n",
    "#     # Save enriched requirements\n",
    "#     save_json(enriched_reqs, \"compliance_enriched_requirements.json\")\n",
    "\n",
    "\n",
    "# show_regulation_coverage(mapped_regs)\n",
    "\n",
    "\n",
    "#     print(\"üìä Step 4: Categorizing requirements...\")\n",
    "#     categorized_reqs = asyncio.run(\n",
    "#         categorize_and_store_requirements(\n",
    "#             PROJECT_ID,\n",
    "#             LLM_MODEL,\n",
    "#             enriched_file=\"enriched_requirements.json\",\n",
    "#             batch_size=batch_size,\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     print(\"üß™ Step 5: Generating test cases...\")\n",
    "#     test_cases = asyncio.run(\n",
    "#         generate_and_store_test_cases(\n",
    "#             PROJECT_ID,\n",
    "#             limit=top_limit,\n",
    "#             batch_size=batch_size\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     print(\"üîó Step 6: Building traceability matrix...\")\n",
    "#     df_trace = build_traceability_matrix(PROJECT_ID)\n",
    "#     print(df_trace.head())\n",
    "\n",
    "#     print(\"‚úÖ Step 7: Running semantic validation...\")\n",
    "#     df_validated = asyncio.run(\n",
    "#         run_semantic_validation(\n",
    "#             PROJECT_ID,\n",
    "#             limit=top_limit,\n",
    "#             batch_size=batch_size\n",
    "#         )\n",
    "#     )\n",
    "#     print(df_validated.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
